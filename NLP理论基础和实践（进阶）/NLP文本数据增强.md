## NLP数据增强方法总结

### 1.传统文本数据增强方法

现有NLP的Data Augmentation大致有两条思路，一个是加噪，另一个是回译，均为有监督方法。**加噪**即为在原数据的基础上通过替换词、删除词等方式创造和原数据相类似的新数据。**回译**则是将原有数据翻译为其他语言再翻译回原语言，由于语言逻辑顺序等的不同，回译的方法也往往能够得到和原数据差别较大的新数据。

1.1**Easy Data Augmentation for Text Classification Tasks （EDA）**提出并验证了几种加噪的 text augmentation 技巧，分别是

- **同义词替换（SR: Synonyms Replace）**

  不考虑stopwords，在句子中随机抽取n个词，然后从同义词词典中随机抽取同义词，并进行替换。

  Eg: “我非常喜欢这部电影” —> “我非常喜欢这个影片”，句子仍具有相同的含义，很有可能具有相同的标签。

- **随机插入(RI: Randomly Insert)**

  不考虑stopwords，随机抽取一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置。该过程可以重复n次。

  Eg : “我非常喜欢这部电影” —> “爱我非常喜欢这部影片”。

- **随机交换(RS: Randomly Swap)**

  句子中，随机选择两个词，位置交换。该过程可以重复n次。

  Eg: “如何评价 2017 知乎看山杯机器学习比赛?” —> “2017 机器学习?如何比赛知乎评价看山杯”。

- **随机删除(RD: Randomly Delete)**

  句子中的每个词，以概率p随机删除。

  Eg: “如何评价 2017 知乎看山杯机器学习比赛?" —> “如何 2017 看山杯机器学习 ”。

  



### 参考

1.[NLP数据增强方法总结：EDA、BT、MixMatch、UDA](https://mp.weixin.qq.com/s/ySxLHnaEMBXjcYPb4xc1Rg)

